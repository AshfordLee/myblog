---
title: 'CS336 Assignment 1 - 手搓BPE和Tranformer'
description: 'CS336 Assignment1 Notes'
publishDate: '2026-1-23'
tags: ['CS336']
# heroImage: { src: './thumbnail.jpg', alt: '作业封面图片' }
# heroImage: {}
language: '中文'
draft: false
---

import { Card, Button } from 'astro-pure/user'

> 从一份TinyStories的故事集到Transformer模型, 就像是从原始人一夜当中走进了现代

# CS336 Assignment 1

这是我做过的第一个(也许也会是最后一个)几乎没什么Skeleton Code的Lab, 这门课的目标是让学习者彻底搞懂大模型的原理，并且从"Scratch"来从头构建大模型。

任务拆分开,大概有这么几点:

首先是工程架构部分:
-   `bpe.py`: Byte-pair encoding(BPE) tokenizer, 在字节级别上实现一个分词器。
-   `transformer.py`: Transformer language model (LM), 实现Transformer的各模块并且组合成可实例化的Transformer类
-   `transformer.py`: The cross-entropy loss function and the AdamW optimizer, 实现AdamW优化器和损失函数
-   `train_transformer.py`: The training loop, with support for serializing and loading model and optimizer state, 实现训练循环

其次是跑训练和测试:
-   `train_bpe_tinystories.py`: Train a BPE tokenizer on the TinyStories dataset. 在Tinystories数据集上训练这个BPE分词器
-   `tokenizer_experiments.py`: Run your trained tokenizer on the dataset to convert it into a sequence of integer IDs. 在数据集上应用分词器, 把文字数据集转化成整数序列
-   `train_transformer.py`: Train a Transformer LM on the TinyStories dataset. 利用分词器的结果训练Transformer模型
-   `TODO_generate_samples.py`: Generate samples and evaluate perplexity using the trained Transformer LM. 用训练好的模型产生结果并且计算困惑度
-   `TODD_train_transformer_OpenWebText.py`: Train models on OpenWebText. 在OpenWebText数据集上训练Transformer模型



## 作业要求

-   `torch.nn.parameter`
-   `torch.nn`
-   `torch.optim.Optimizer`(作为基类) 

这个作业主要还是让学习者实现算法和工程架构,至于底层那些并行计算之类的,交给PyTorch的开发者去做吧

## 评测框架

没有给出直接一键运行的测试,虽然测试的输入和输出ASSERT是实现好的,但是要自己去接这个测试接口,测试逻辑在`./assignment1-basics/tests`里面,测试接口在`./assignment1-basics/tests/adapters.py`里面

比如在`transformer.py`我们实现了Linear Layer,对应的测试接口在:

```python
# Test For Implement of Linear Layer
from cs336_basics import transformer

def run_linear(
    d_in: int,
    d_out: int,
    weights: Float[Tensor, " d_out d_in"],
    in_features: Float[Tensor, " ... d_in"],
) -> Float[Tensor, " ... d_out"]:
    """
    Given the weights of a Linear layer, compute the transformation of a batched input.

    Args:
        in_dim (int): The size of the input dimension
        out_dim (int): The size of the output dimension
        weights (Float[Tensor, "d_out d_in"]): The linear weights to use
        in_features (Float[Tensor, "... d_in"]): The output tensor to apply the function to

    Returns:
        Float[Tensor, "... d_out"]: The transformed output of your linear module.
    """

    # raise NotImplementedError
    linear_module = transformer.Linear(in_features=d_in,out_features=d_out)
    linear_module.weight.data = weights
    return linear_module(in_features)

```
`transformer.Linear`是我们实现的Linear类,实现这个接口之后，运行
```bash
uv run pytest -k test_linear
```
就可以运行预设的测试了

## 下载数据
有四个数据集,两个Train两个Valid,分别对应TinyStories和OpenWebText_Result数据集
``` bash
mkdir -p data
cd data

wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz
gunzip owt_train.txt.gz
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz
gunzip owt_valid.txt.gz

cd ..
```
如果有网络问题，可以从`https://hf-mirror.com/`下载,替换下载命令里面的地址就行了

## 环境配置
本lab用uv做包管理器,我之前没用过完全不熟练,我是直接照着他文档里面的命令安装的,先安装uv
```bash
pip install uv
```

当运行某个py文件的时候,用uv命令运行,如果有依赖缺失会自动下载
```bash
uv run <python_file_path>
```

注意给uv也像pip一样配置一个国内源,不然安装一些大包可能要安装到明天早上去
```bash
# 推荐使用清华源
echo 'export UV_DEFAULT_INDEX="https://pypi.tuna.tsinghua.edu.cn/simple"'>> ~/.bashrc

# 或者用阿里源
# echo 'export UV_DEFAULT_INDEX="https://mirrors.aliyun.com/pypi/simple/"' >> ~/.bashrc

# 让配置立即生效
source ~/.bashrc
# 转载自https://zhuanlan.zhihu.com/p/1930714592423703026
```

## BPE算法
文档中给出一个例子(stylized example),考虑以下的语料
```
low low low low low
lower lower widest widest widest
newest newest newest newest newest newest
```

我们有一个词语集合Vocabulary(代码里一般称为Vocab),可以理解为是一个词汇表，这个词汇表一开始内容很少,然后在训练的时候慢慢增长

假如我们通过whitespace来对语料进行分割,我们就可以得到frequency table:
```
{low: 5, lower: 2, widest: 3, newest: 6}
```

但我们要换一种更方便的数据结构来表示,比如用`dict[tuple[bytes], int]`,那么这个表被表示成:
```
{(l,o,w):5,(l,o,w,e,r):2, ...}
```

接下来我们统计这个frequency table里面byte(char)对的两两组合,这就是个计数的工程,得到的结果是:
```
{lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}
```

找到那些出现次数最多的组,这个例子里面是
```
{es:9, st:9}
```
挑选字典序更大的那个,这里是st,把所有的's' 't' 组合成st,所以这个frequency table就变成了
```
 {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}
```
然后再把'st'添加到vocab里面去

再重复上面这个那个计数的步骤,这一次'e' 'st'是出现最频繁的,所以对他进行合并

如果我们重复合并到没有可以继续的了,我们的merges应该是:
```
['s t', 'e st', 'o w', 'l ow', 'w est', 'n e',
'ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r']
```
其中的每一项代表我们在那一次合并当中把两个什么东西(byte)合并了

当然实际上并不一定要合并到最后,我们可以指定合并的次数,比如例子中指定为6次,那么merges应该是:

```
['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']
```

这种情况下我们的vocab将会变成
```
[<|endoftext|>, [...256 BYTE CHARS], st, est, ow, low, west, ne]
```
其中前面两个是init vocab的时候就已经有的,然后每一次merge的时候会往vocab里面加一项

如此一来的话,词语newest就会被分词成为'ne' 'west', 换言之这个merge的过程就是把分词这件事情从细变粗的过程,最开始每一个词语都是一个一个字母分的,现在有一部分字母被聚合了


## 并行预分词  Parallelizing pre-tokenization

### chunk和对每个chunk的处理
首先要对原有的文本进行预分词,以TinyStories数据集为例:
```
u don't have to be scared of the loud dog, I'll protect you". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.
<|endoftext|>
Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.
<|endoftext|>

They went back to the living room and cleaned up their toys. They decided to build something together. They made a big house with a garden and a fence. They put their cars and dolls inside. They were happy and proud of their work.
Mommy and Daddy came to see their house. They praised them and gave them a treat. It was a lemon cake. It was sour, but they liked it. They learned that sharing is caring, and that family is sweet.
<|endoftext|>


Lucy and the little girl played together happily. In the end, they both learnt an important lesson: be peaceful, kind, and understanding when faced with a conflict. And that is why Lucy and the little girl became great friends.
<|endoftext|>

At the end of the day, Tom and Max were tired. They had played all day and had lots of fun. They said goodbye to each other and went to their homes. Before going to sleep, they both did another easy stretch. Tom knew that tomorrow would be another happy morning.
<|endoftext|>
```

忽略数据的内容,注意到故事和故事之间是用`<|endoftext|>`进行分割的,所以我们把两个`<|endoftext|>`之间的内容成为一个chunk,首先要把chunk提取出来并且得到每一个chunk的`frequency_dict`

课程组给了一段现成的代码,在`./assignment1-basics/cs336_basics/pretokenization_example.py`里面,这个函数的作用是读取原始的Corpus,然后把chunk的边界,大概约等于上面说的`<|endoftext|>`的位置返回

```python
def find_chunk_boundaries(
    file: BinaryIO,
    desired_num_chunks: int,
    split_special_token: bytes,
) -> list[int]:
```
注意这个分割字符(在这个例子里面是`<|endoftext|>`),是不唯一的,当然有可能某个corpus的是用`<|Hello|>`来区分段落的,只要数据类型是`bytes`就行了

既然需要并行,我们首先要写处理单个chunk的函数:构造如下

```python
def pretokenize_chunk(args): # Deal with a chunk
    # bpe.py

    start,end,input_path,split_special_token = args
    # start: chunk的开始索引
    # end: chunk的结束索引
    # input_path: 文件目录
    # split_special_token: chunk内的分隔符
    
    with open(input_path,"rb") as f: # 打开文件
        f.seek(start) # 文件指针移动start偏移量(将文件读取指针移动到 start 指定的字节位置)
        chunk_bytes = f.read(end - start).decode("utf-8", errors="ignore") 
        # 读取start到end之间的数据
        
    split_pattern = "|".join(re.escape(token) for token in split_special_token)
    # 现在这个split_pattern能够正则匹配任意的split_special_token内的分隔符
    text_segments = re.split(f"({split_pattern})",chunk_bytes)
    # 对文本进行分割, 返回交替的结果, 形如[文本段1, 分隔符1, 文本段2, 分隔符2, ...]

    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    # GPT-2 风格的正则分词


    frequency_dict = defaultdict(int) # 初始化频率字典,注意用defaultdict来确保默认值为0,避免后面访问下标不存在的问题

    for segment in text_segments: # 遍历文本段
        if segment not in split_special_token: # 跳过分隔符,只处理实际文本

            for match in re.finditer(PAT,segment): # 在这个实际文本当中找所有的匹配项,返回迭代器
                pretoken = match.group()
                pretoken_bytes = pretoken.encode("utf-8") # 编码成UTF-8
                pretoken_bytes_tuple = tuple(bytes([b]) for b in pretoken_bytes)
                # 转化成字节tuple,例如("h", "e", "l", "l", "o") 而不是 b"hello"

                frequency_dict[pretoken_bytes_tuple] += 1
                # 更新频率


    return dict(frequency_dict)
```
要注意文本处理的架构(层次)是: `整个语料`->`单个chunk`->`chunk中的一个segment`->`segment当中的每个文本`

如果`split_special_token = [b"<|endoftext|>"]`,文本为:
```
Story 1 content here.<|endoftext|>Story 2 content here.<|endoftext|>
```
那么经过分割后的`text_segments`为
```
[
    "Story 1 content here.",  # 文本段
    "<|endoftext|>",          # 分隔符（被保留）
    "Story 2 content here.",  # 文本段
    "<|endoftext|>"           # 分隔符（被保留）
]
```
然后再遍历这个text_segments,得到的结果格式类似于(不一定准确,只是形式上类似,具体还要看这个PAT对每个segment的分割规则):
```
{
    (b'S', b't', b'o', b'r', b'y'): 2,  # "Story" 出现2次
    (b' ',): 6,                          # 单个空格出现6次
    (b'1',): 1,                          # "1" 出现1次  
    (b'2',): 1,                          # "2" 出现1次
    (b'c', b'o', b'n', b't', b'e', b'n', b't'): 2,  # "content" 出现2次
    (b'h', b'e', b'r', b'e'): 2,         # "here" 出现2次
    (b'.',): 2,                          # "." 出现2次
}
```
这就完成了对每个chunk的内部切割,接下来需要一个并行代码把每个chunk映射到这个函数上

### 同时并行处理多个chunk
我们应当用多进程同时处理多个chunk,每个chunk会独立的返回这个chunk内容的`frequency_dict`,然后把他们concat到一起去就得到了整个语料的频率字典

```python
def load_and_chunk_file(
    input_path: str,
    desired_num_chunks: int,
    split_special_token: list[str],
    Debug=False): 
    # bpe.py

    with open(input_path,"rb") as f: # 打开文件
        num_processes = 16 # 设定并行的进程数量

        split_token_bytes = split_special_token[0].encode("utf-8") # 拿到分隔符并且把他encode成bytes

        boundaries = find_chunk_boundaries(file=f,desired_num_chunks=desired_num_chunks,split_special_token=split_token_bytes)
        # 调用给出的find_chunk_boundaries函数, 这里返回的就是原始语料里面每个分隔符的位置,在这个项目里面就是每个<|endoftext|>的位置


    with multiprocessing.Pool(processes=num_processes) as pool: # 并行处理
        
        chunk_args=[(start,end,input_path,split_special_token) for start,end in zip(boundaries[:-1],boundaries[1:])]
        # 四个参数, 注意boundaries的数据类型是list[int], 从boundaries[0,1]开始滑动取得每一组start和end
        # input_path和split_special_token都是不变的

        results=pool.map(pretokenize_chunk,chunk_args)
        # 传入参数并且获得并行传回的结果


        total_frequencies = merge_frequencies(results)
        # 把结果"concat"起来

        return total_frequencies
```

注意这个`concat`其实不是真的`concat`, 因为不同的chunk里面有可能出现相同的词语,比如在chunk1和chunk2的`frequency_dict`里面也许都有
```
{(b'S', b't', b'o', b'r', b'y'): 2}
```
那显然concat之后得到的应该是:
```
{(b'S', b't', b'o', b'r', b'y'): 4}
```
所以需要对并行传回来的结果进行遍历,然后合并同类项
```python
# bpe.py
def merge_frequencies(frequency_dict): # Calculate the frequencies from each chunks and sum them together

    total_frequencies = defaultdict(int) # 初始化结果

    for every_frequency_dict in frequency_dict: 
    # 遍历每个chunk的frequency_dict
        for pretoken_bytes, count in every_frequency_dict.items():

            total_frequencies[pretoken_bytes] += count
            # 保证不同chunk之间的同样的pretoken_bytes的计数不重不漏

    return total_frequencies
```

### 初始化Vocab和Merges
从前面那个例子可以看到, 训练过程本质上就是更新Vocab和Merges这两个结果的过程, 所以先对他们进行初始化
```python
# bpe.py
def initialize_vocab_and_merges(special_tokens):
    vocab = {}

    for i in range(256):
        vocab[i] = bytes([i]) # 初始的一些默认bytes

    for special_token in special_tokens:
        vocab[len(vocab)] = special_token.encode("utf-8") # 我们自己附加的special_tokens

    return vocab,[]
```
随着训练进行, Vocab和Merges会越来越大

### 初始化pair计数
我们现在只得到了每个词语的计数, 但是如例子里面, 我们要对每个词语遍历拆出pair, 去对pair进行计数, 比如说输入是
```
{(b'S', b't', b'o', b'r', b'y'): 4}
```
那输出大概是
```
{'St':4, 'to':4, 'or':4, 'ry':4}
```
注意这里已经不分chunk了, 所以每次得到的结果就是全局的更新量了

```python
#bpe.py
def get_initial_pair_frequencies(frequency_dict,Debug=False):

    pair_freq = defaultdict(int)
    pair_to_tokens = defaultdict(set)
    # 初始化

    for pretoken_bytes, count in frequency_dict.items():
        # 遍历词频字典, 获取每个"单词"和他的计数


        for i in range(len(pretoken_bytes)-1): # 遍历这个单词的所有相邻字符
            pair = ((pretoken_bytes[i],), (pretoken_bytes[i+1],)) # 取得相邻的pair
            pair_freq[pair] = pair_freq.get(pair,0) + count # 更新这个pair的计数
            pair_to_tokens[pair].add(pretoken_bytes) # 记住某一个pair出现在哪个单词里面


    return pair_freq, pair_to_tokens
```
## 结果展示

展示您的作业结果...

## 总结

总结这次作业的收获...