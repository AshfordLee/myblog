---
title: 'UC Berkeley CS189 Assignment 1(Part 1)'
description: 'CS189 Assignment1 Notes'
publishDate: '2026-1-26'
tags: ['CS189']
# heroImage: { src: './thumbnail.jpg', alt: '作业封面图片' }
# heroImage: {}
language: '中文'
draft: true
---

import { Card, Button } from 'astro-pure/user'

# CS189 Assignment1

189这门课感觉也是理论和实践并行教学, Written Part的作业也不少, 不过比起336那种工程性质极强的课程来说还是容易不少, 至少这第一个Assignment没有和336那样, 一上来就整一大堆手搓, 189的作业基本上就是理解原来并调包, 极少的自己实现, 非常适合有志于成为API工程师的人学习

Written Part的数学作业也并不容易, 后面有机会写一些Notes


## 作业要求

基本上是Fill in the Blanks性质的Lab, 把函数里面的所有TODO填满就行了, 几乎不需要自己设计接口

## 评测框架

用的是UC Berkeley自行设计的`otter`框架, 一键安装就行了:
```bash
pip install otter-grader
```

## 环境配置

每个`hw`文件当中都给了`requirements.txt`, 直接安装就行
```bash
pip install -r requirements.txt
```

除此之外就是配置一下网络环境, 由于我是在GPU服务器上运行的, 所以一些没找到镜像站的数据下载代码我就用反向ssh端口让他走我本地的代理, 打开本地终端开启这个反向ssh:
```bash
(base) liziyu@liziyudeMacBook-Pro ~ % ssh -R 7891:localhost:7890 zyli@lab
Welcome to Ubuntu 24.04.3 LTS (GNU/Linux 6.14.0-29-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

Expanded Security Maintenance for Applications is not enabled.

124 updates can be applied immediately.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status

*** System restart required ***
Last login: Fri Jan 23 15:16:55 2026 from 61.169.135.222
(base) zyli@lab:~$ 
```

然后在notebook里面通过os来配置一下端口:
```python
import os
where="lab"
# where="local"

if where=="lab":

    os.environ['HTTP_PROXY'] = 'http://localhost:7891'
    os.environ['HTTPS_PROXY'] = 'http://localhost:7891'
```

这里其实是比较烦的, 我本人还是喜欢去运行py文件, 用`ALL_PROXY=localhost:7891`的形式去做

## Assignment Overview
第一部分的所有代码在`fashion_pt_1.ipynb`里面, 都是一些比较基础的`pandas`和`numpy`的操作, 还有一些画图之类的, 主要就是让学习者习惯数据预处理

## 加载`Fashion-MNIST`数据集

搞清楚数据集本身作为一个对象有什么属性就行
```python
# Load the FashionMNIST dataset from torchvision
train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)

# Extract the image data and convert it to a numpy array of type float
images = train_data.data.numpy().astype(float)

# Extract the target labels as a numpy array
targets = train_data.targets.numpy()

# Create a dictionary mapping class indices to class names
class_dict = {i: class_name for i, class_name in enumerate(train_data.classes)}

# Map the target labels to their corresponding class names
labels = np.array([class_dict[t] for t in targets])

# Create a list of class names in order of their indices
class_names = [class_dict[i] for i in range(len(class_dict))]

# Get the total number of samples in the dataset
n = len(images)

# Ensure class_names is a list of class names (redundant but ensures consistency)
class_names = list(class_dict.values())

# Print dataset information for verification
print("Loaded FashionMNIST dataset with {} samples.".format(n))
print("Classes: {}".format(class_dict))
print("Image shape: {}".format(images[0].shape))  # Shape of a single image
print("Image dtype: {}".format(images[0].dtype))  # Data type of the image array
print("Image type: {}".format(type(images[0])))   # Type of the image object
```

```
Loaded FashionMNIST dataset with 60000 samples.
Classes: {0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}
Image shape: (28, 28)
Image dtype: float64
Image type: <class 'numpy.ndarray'>
```

## Problem 0a
从数据集的两列`numpy.series`:`images`和`targets`构造`DataFrame`
```python
# TODO: Create a DataFrame with two columns: `image` and `label`

...
image_list=images.tolist()
label_list=labels.tolist()

df=pd.DataFrame({'image':image_list,'label':label_list})
df['image']=df['image'].apply(np.array)

# Print the shape and columns of the DataFrame
print("DataFrame shape:", df.shape)
print("DataFrame columns:", df.columns.tolist())
df.head()
```
```
DataFrame shape: (60000, 2)
DataFrame columns: ['image', 'label']

```

## Problem 1a
计算数据集每个`label`出现的个数, 然后看每个`label`出现的个数是否相等
```python
# TODO: Calculate the distribution of labels using `value_counts()``
# TODO: Compare the min and max values of `label_distribution` to determine if the dataset is balanced. 

label_distribution=df['label'].value_counts()
is_balanced=label_distribution.min()==label_distribution.max()

print(f"Label distribution:\n{label_distribution}")
print(f"Is the dataset balanced? {is_balanced}")
```

```
Label distribution:
label
Ankle boot     6000
T-shirt/top    6000
Dress          6000
Pullover       6000
Sneaker        6000
Sandal         6000
Trouser        6000
Shirt          6000
Coat           6000
Bag            6000
Name: count, dtype: int64
Is the dataset balanced? True
```

## Problem 1b
用`groupby()`分组并且统计每个组的行数

```python
# TODO: Group the rows in `df` according to the values in the `labels` column. Then, count the number of rows in each group.
label_distribution_groupby = df.groupby('label').size()

label_distribution_groupby
```

```
label
Ankle boot     6000
Bag            6000
Coat           6000
Dress          6000
Pullover       6000
Sandal         6000
Shirt          6000
Sneaker        6000
T-shirt/top    6000
Trouser        6000
dtype: int64
```

## Problem 1c 
对label列进行可视化

```python
# Plotting library to use, default is matplotlib but plotly has more functionality
pd.options.plotting.backend = "plotly" 

# TODO: Plot a histogram of the labels in the DataFrame `df` using the DataFrame's built-in plotting functions (this should be 1 line)
df['label'].plot(kind="hist")
```
<img src="/images/189_hw1_1.png" alt="label_plot" />

课程组还给了一个`show_images`函数, 他会打出图片和`label`
```python
def show_images(images, max_images=40, ncols=5, labels = None, reshape=False):
    """Visualize a subset of images from the dataset.
    Args:
        images (np.ndarray or list): Array of images to visualize [img,row,col].
        max_images (int): Maximum number of images to display.
        ncols (int): Number of columns in the grid.
        labels (np.ndarray, optional): Labels for the images, used for facet titles.
    Returns:
        plotly.graph_objects.Figure: A Plotly figure object containing the images.
    """
    if isinstance(images, list):
        images = np.stack(images)
    n = min(images.shape[0], max_images) # Number of images to show
    px_height = 220 # Height of each image in pixels
    if reshape:
        images = images.reshape(images.shape[0], 28, 28)
    fig = px.imshow(images[:n, :, :], color_continuous_scale='gray_r', 
                    facet_col = 0, facet_col_wrap=ncols,
                    height = px_height * int(np.ceil(n/ncols)))
    fig.update_layout(coloraxis_showscale=False)
    fig.update_xaxes(showticklabels=False, showgrid=False)
    fig.update_yaxes(showticklabels=False, showgrid=False)
    if labels is not None:
        # Extract the facet number and replace with the label.
        fig.for_each_annotation(lambda a: a.update(text=labels[int(a.text.split("=")[-1])]))
    return fig
```
后面会用到

## Problem 1d
用`groupby`分类, 然后在每个分类里面挑两张图, 打印出图和他的分类
```python
# TODO: Get 2 sample images per class and plot them.
examples = df.groupby('label').head(2)

fig = show_images(examples["image"].tolist(), ncols=4, labels=examples["label"].tolist())
fig.show()
```

<img src="/images/189_hw1_2.png" alt="label_plot" />

## Problem 2
用`reshape`函数把一个`m*n`的图像变成`mn*1`的
```python
df["image"] = df["image"].apply(lambda img: img.reshape(-1))
```
原始的每行的`image`元素都是`28*28`的, 现在每行都变成`784*1`的了

```python
np.stack(df['image'].values).shape
```

```
(60000, 784)
```

这里说一下这个`np.stack(arrays, axis, out)`的用法, 以前每次都是AI写, 不太清楚其中原理

`np.stack`作用于一些数组, 这些数组必须有相同的shape, 沿着axis指定的轴拼起来, 这会加入第n+1个维度, 就像两本书叠起来, 显然要有一个z轴来标识这是第一本书还是第二本书

```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
np.stack((a, b))
# array([[1, 2, 3],
#        [4, 5, 6]]) 2*3, 第0维出现一个2

np.stack((a, b), axis=-1)
# array([[1, 4],
#        [2, 5],
#        [3, 6]]) 3*2, 第1维出现一个3
```
简而言之就是axis是多少, 结果在第axis个维度上就会出现一个n, n是数组的个数

## Problem 2a
调用`sklearn.KMeans`进行聚类

```python
# TODO: Perform k-means clustering on the images (10 clusters to match the number of classes)
from sklearn.cluster import KMeans

df_sample = df.sample(n=1000, random_state=SEED)
kmeans=KMeans(n_clusters=10,random_state=SEED)
X=np.stack(df_sample['image'].to_numpy())
cluster_labels=kmeans.fit_predict(X)

kmeans_df=df_sample.assign(cluster=cluster_labels)

kmeans_df.head(3)
```
这里的np.stack就是把一堆图片点聚集起来, 现在这个kmeans_df有了一个新的列cluster, 代表聚类的结果, 比如说会把一些行聚类成1类, 2类....以此类推, 总共10个类

## Problem 2b
评估KMeans聚类的效果

只需要对label(聚类的结果)groupby一下然后看每个聚类里面有多少种不同的`true_label`, 如果聚类的效果比较好的话, 应该每种聚类里面有尽可能少的`true_label`种类

```python
# TODO: Create a stacked bar plot of the label counts per cluster.
cluster_label_counts = kmeans_df.groupby(['cluster', 'label']).size().unstack(fill_value=0)

cluster_label_counts.plot(
    kind='bar',
    title='Distribution of True Labels in Each K-means Cluster'
)
```
<img src="/images/189_hw1_3.png" alt="label_plot" />

上面的数据操作有点烦人, 一不小心就会写错, 这其实是一个类似于`pivot`的操作, 经过groupby(["cluster","label"]).size()之后会有一个二级索引
```
    cluster  label
    0        0         50
             1         12
             2         5
    1        0         10
             2         80
    ...
```

unstack之后会把内层索引`index`变成列名, 这就创建了`(cluster,label)`到`count`的一一映射, 然后就可以画图了
```
cluster  label0  label1  label2  
0        50      12      5       
1        10      80      0       
...
```
这样一来`count`就成了没名字的元素了

## Problem 2c
在每个Cluster里面随机抽出7个图片, 把他们打出来

```python
# TODO: Plot 7 images from each cluster (use the show_images function, 10 rows, 7 columns)
Cluster_Head=kmeans_df.groupby('cluster').sample(n=7,random_state=SEED)

fig = show_images(Cluster_Head["image"].tolist(), ncols=7, labels=Cluster_Head["cluster"].tolist(),reshape=True,max_images=70)
fig.show()

# Cluster_Head
```
<img src="/images/189_hw1_4.png" alt="label_plot" />
可以看到聚类的效果还是不错的, 至少没把衣服和鞋子放一起去

## Problem 3
 
训练一个MLP分类器, 总共四个步骤

-  `数据预处理`
-  `模型训练`
-  `绩效评估`
-  `可视化`

先进行训练集和测试集的切分, 不能用`train_test_split`, 其实也很简单, 先sample出训练集, 那不在训练集里面的就是测试集
```python
df_copy = df.copy()
train_df = df_copy.groupby('label').sample(frac=0.8, random_state=SEED)
test_df = df_copy[~df_copy.index.isin(train_df.index)]
print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")
```

## Problem 3a

展平并归一化数据点, 启动训练并且绘制损失曲线

```python
# flatten features into 1D arrays
X_train = np.stack(train_df['image'].values)
y_train = train_df['label'].values
X_test = np.stack(test_df['image'].values)
y_test = test_df['label'].values

print(f"X_train shape: {X_train.shape}\t y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}\t y_test shape: {y_test.shape}")

# TODO: Train the model using the scaled traning data and plott the loss curve (remeber to normalize your data!)
# NOTE: Your model must be named `model`
def flatten(images):
    return images.reshape(images.shape[0], -1)
image_scalar=StandardScaler()
image_scalar.fit(flatten(X_train))
X_train_sc=image_scalar.transform(flatten(X_train))
X_test_sc=image_scalar.transform(flatten(X_test))

if load_saved_models and os.path.exists('classifier.joblib'):

    model = joblib.load('classifier.joblib')

else:

    model=MLPClassifier(
    hidden_layer_sizes=(100, 50),
    max_iter=200, tol=1e-4, random_state=SEED)

    model.fit(X_train_sc,y_train)
if save_models:
    joblib.dump(model, 'classifier.joblib')

loss_df=pd.DataFrame({'epoch':np.arange(1,len(model.loss_curve_)+1),'loss':model.loss_curve_})
loss_df.plot(x='epoch', y='loss', title="Training Error")
```

从原始的`DataFrame`里面拿数据出来的时候遵循先`stack`再`reshape`, 感觉很多时候脑子里面并没有对数据的具体形状有一个清晰的记忆, 尤其是在高维张量的情况下, 但是遵照这个去做一般不会出错

```
X_train shape: (48000, 784)	 y_train shape: (48000,)
X_test shape: (12000, 784)	 y_test shape: (12000,)
```

<img src="/images/189_hw1_5.png" alt="label_plot" />

## Problem 3b
产生预测结果和Evaluation Metrics, 这里主要就是一些API的调用, 主要涉及到`Sklearn.model`的属性怎么用的问题

```python
# TODO: Add the columns listed above to `train_df` and `test_df`.
train_df = train_df.copy()
test_df = test_df.copy()

train_predict=model.predict(X_train_sc)
test_predict=model.predict(X_test_sc)
```

`model.predict`返回`(n_samples,)`的np数组, 所以他是可以直接和真实的label数组去进行比较的

```python
train_correct=(train_predict==y_train)
test_correct=(test_predict==y_test)
```

分类模型本质上在预测概率, 对于每个class, 给出一个prob_class, 所以`model.predict_prob`会给出`(n_samples,n_classes)`的数组, 即对于每个样本的每个潜在分类给出一个概率

```python
train_probs=model.predict_proba(X_train_sc)
test_probs=model.predict_proba(X_test_sc)
```

接下来还要得到每个样本的置信度, 实际上就是每个样本预测概率当中最大的那个概率, 通俗来说, 如果有十个候选类, 然后其中预测的最大概率是100%, 其他的类的概率都是0%, 那至少看起来是比较可信的
```python
train_confidence=np.max(train_probs,axis=1)
test_confidence=np.max(test_probs,axis=1)
```
以上均得到`(n_samples,)`的数组

接下来把这些贴回到原来的df上去

```python
train_probs_list=[list(probs) for probs in train_probs]
test_probs_list=[list(probs) for probs in test_probs]

train_df['predicted_label']=train_predict
train_df['correct']=train_correct
train_df['probs']=train_probs_list
train_df['confidence']=train_confidence

test_df['predicted_label']=test_predict
test_df['correct']=test_correct
test_df['probs']=test_probs_list
test_df['confidence']=test_confidence
```

最后算一下准确率就行了

```python
print("--- Column Types ----")
for col in train_df.columns:
    val = train_df[col].iloc[0]
    print(f"{col}: {type(val)}")
print("-----------")


train_accuracy = train_correct.sum()/len(train_correct)
test_accuracy = test_correct.sum()/len(test_correct)

print(f"Training accuracy: {train_accuracy:.3f}")
print(f"Test accuracy: {test_accuracy:.3f}")
```

```
--- Column Types ----
image: <class 'numpy.ndarray'>
label: <class 'str'>
predicted_label: <class 'str'>
correct: <class 'numpy.bool'>
probs: <class 'list'>
confidence: <class 'numpy.float64'>
-----------
Training accuracy: 0.993
Test accuracy: 0.883
```

## Problem 3c
分组查看准确率, 只要`groupby('label')['correct'].mean()`就可以得到每个类的准确率, 然后再把train和test两张表`concat`起来, 最后用`pivot`把它变成宽表即可

```python
# TODO: Calculate train and test accuracy per class 
# TODO: Use class_accuracy to create a grouped bar chart of class accuracy for train and test

train_class_acc=train_df.groupby('label')['correct'].mean().reset_index()
train_class_acc['split']='train'

test_class_acc=test_df.groupby('label')['correct'].mean().reset_index()
test_class_acc['split']='test'

all_df=pd.concat([train_class_acc,test_class_acc])

class_accuracy=all_df[['split','label','correct']]
# print(class_accuracy)

class_accuracy_pivot=class_accuracy.pivot(index='label',columns='split',values='correct')
print(class_accuracy_pivot)

class_accuracy_pivot.plot(kind='bar',text_auto=True)
```

```
split            test     train
label                          
Ankle boot   0.967500  0.998333
Bag          0.966667  0.999792
Coat         0.807500  0.993333
Dress        0.888333  0.995208
Pullover     0.810000  0.990208
Sandal       0.945833  0.998958
Shirt        0.737500  0.990833
Sneaker      0.932500  0.996458
T-shirt/top  0.802500  0.967083
Trouser      0.975833  0.997917
```

<img src="/images/189_hw1_6.png" alt="label_plot" />

## Problem 3d
计算不同类别之间的预测效果, 首先要用np算一个`10*10`的`confusion matrix`, 行代表真实label, 列代表预测label, 每个元素代表这种预测的个数, 显然, 对角线上的元素求和就是总共预测对的数量

```python
# Initialize confusion matrix with zeros
conf_matrix = np.zeros((len(class_names), len(class_names)), dtype=int)
class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}

# Fill the confusion matrix by counting predictions and plot it as a heatmap
y_true=test_df['label'].values
y_pred=test_df['predicted_label'].values

true_indices = np.array([class_to_idx[label] for label in y_true])
pred_indices = np.array([class_to_idx[label] for label in y_pred])

np.add.at(conf_matrix, (true_indices, pred_indices), 1)

fig=px.imshow(conf_matrix,
                labels=dict(x="Predicted Label", y="True Label", color="Count"),
                x=class_names,
                y=class_names,
                )

fig.show()
```
实际上就是遍历一遍真实label和预测label, 注意要把label的名字转成int, 然后对矩阵的`(int,int)`加一就好了

<img src="/images/189_hw1_7.png" alt="label_plot" />

```python
conf_matrix
```

```
array([[ 963,    0,   23,   31,    1,    4,  171,    0,    7,    0],
       [   4, 1171,    4,   15,    1,    0,    4,    0,    1,    0],
       [  20,    2,  972,   14,  102,    0,   86,    0,    4,    0],
       [  27,    5,   19, 1066,   37,    1,   43,    0,    2,    0],
       [   7,    1,  112,   28,  969,    0,   79,    0,    4,    0],
       [   2,    0,    0,    3,    0, 1135,    0,   35,    5,   20],
       [ 118,    2,   96,   19,   71,    0,  885,    1,    8,    0],
       [   0,    0,    0,    0,    0,   24,    0, 1119,    1,   56],
       [   6,    1,    5,    2,    7,    4,   12,    2, 1160,    1],
       [   0,    0,    0,    0,    0,   10,    0,   27,    2, 1161]])
```

接下来需要计算FP, FN, TP指标, 直接在这个矩阵上操作即可, 只需要注意对角线上是对的, 其他的都是错的

```python
# Calculate accuracy from confusion matrix
accuracy_from_matrix = np.trace(conf_matrix)/np.sum(conf_matrix)
print(f"\nAccuracy calculated from confusion matrix: {accuracy_from_matrix:.3f}")

# Calculate per-class metrics from confusion matrix
per_class_metrics = []
print("\nPer-class metrics from confusion matrix:")
for i, class_name in enumerate(class_names):
    true_positives = conf_matrix[i][i]
    false_positives = np.sum(conf_matrix[:,i])-conf_matrix[i][i]
    false_negatives = np.sum(conf_matrix[i,:])-conf_matrix[i][i]
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
    per_class_metrics.append({
        'class': class_name,
        'precision': precision,
        'recall': recall
    })
    
pd.DataFrame(per_class_metrics)
```

```
Accuracy calculated from confusion matrix: 0.883

Per-class metrics from confusion matrix:
```
<img src="/images/189_hw1_8.png" alt="label_plot" />



## Problem 3f

找出低置信度的预测并且plot出来

```python
# TODO: Find the image with the lowest confidence by sorting the `confidence` column of `test_df`
least_confident = test_df.sort_values(by='confidence',ascending=True)
print("Image with lowest confidence:")
print(least_confident[['label', 'predicted_label', 'confidence', 'correct']][:3])

# Show image with lowest confidence and its predicted label
show_labels = [f"{label} (Pred: {predicted_label})" for label, predicted_label in zip(least_confident["label"].tolist(), least_confident["predicted_label"].tolist())]
fig = show_images(np.stack(least_confident["image"].tolist()), 8, ncols=4, labels=show_labels, reshape=True)
fig.show()
```

```
Image with lowest confidence:
         label predicted_label  confidence  correct
20252     Coat           Shirt    0.385341    False
23752  Sneaker      Ankle boot    0.395444    False
18719  Sneaker      Ankle boot    0.396654    False
```

<img src="/images/189_hw1_9.png" alt="label_plot" />

可以看到, 在置信度非常低的情况下, 很多都预测错了


## Problem 3g

找一些置信度很低但是预测对了的的Ankle boot类并画图, 非常容易, 先从`test_df`筛选出`label == 'Ankle boot' & correct == True`的, 然后用`show_images`画图就行了

```python
# TODO: Visualize 10 images from the `test_set` whose true label is `Ankle boot` that the model correctly classified but with low confidence
test_df_boot = test_df[(test_df['label']=='Ankle boot')  &  (test_df['correct']==True)].sort_values(by='confidence',ascending=True).head(10)
# 将 labels 转换为列表，或者使用 .tolist()
show_labels = [f"{label} " for label in test_df_boot['label'].tolist()]
fig = show_images(np.stack(test_df_boot["image"].tolist()), 10, ncols=5, labels=show_labels, reshape=True)

fig.show()
```
<img src="/images/189_hw1_10.png" alt="label_plot" />


## Problem 3l
再找一些trouser类当中置信度很高但预测错误的画图, 和上面的一样, 照猫画虎即可
```python
# TODO: Visualize 10 images from the `test_set` whose true label is `Trouser` that the model incorrectly classified as `Dress` with high confidence
test_df_trouser = test_df[(test_df['label']=='Trouser')  &  (test_df['correct']==False)].sort_values(by='confidence',ascending=False).head(10)
show_labels = [f"{label} " for label in test_df_trouser['label'].tolist()]
fig = show_images(np.stack(test_df_trouser["image"].tolist()), 10, ncols=5, labels=show_labels, reshape=True)

fig.show()
```
<img src="/images/189_hw1_11.png" alt="label_plot" />

可以看到, 就算置信度很高, 也会有非常离谱的错误, 比如说把一个人分类成裤子, 从结果去理解过程, 我觉得他识别出的特征是两条裤腿


## Problem 4

这一部分作业要求我们实现各种各样的图像增强, 我个人觉得这是比较难的部分, 因为这一段的`numpy`调用很多, 一不小心就不记得数据处理成什么样了

首先实现一些功能函数, 在高等代数里面学过, 很多变换(线性)都可以通过矩阵乘法来表示, 所以实现这个应用变换的函数

```python
def apply_transformation(image, T):
    # Input: A (N, 784) image vector and a (784, 784) transformation matrix
    # Output: A (N, 784) image vector
    transformed_flat = image @ T.T
    return transformed_flat.reshape(image.shape)
```

即将变换矩阵作用在图像上, 得到新图像

接下来是一个例子, 告诉我们如何实现图像的上下颠倒

```python
def create_vertical_flip_matrix(height=28, width=28):
    """
    Returns a (height*width, height*width) matrix that vertically flips an image
    when applied to its flattened vector. Values are 0 or 1.
    """
    N = height * width  # Total number of pixels in the image
    T = np.zeros((N, N), dtype=int)  # Initialize the transformation matrix with zeros
    for i in range(height):  # Loop over each row
        for j in range(width):  # Loop over each column
            orig_idx = i * width + j  # Compute the flattened index for the original pixel
            flipped_i = height - 1 - i  # Compute the row index after vertical flip
            flipped_idx = flipped_i * width + j  # Compute the flattened index for the flipped pixel
            # Set the corresponding entry in the transformation matrix to 1
            # This means the pixel at (i, j) moves to (flipped_i, j)
            T[flipped_idx, orig_idx] = 1
    return T

def vertical_flip(image):
    T_flip = create_vertical_flip_matrix()
    return apply_transformation(image, T_flip)
```

上下颠倒只改变行, 不改变列, 实际上就是把第`i`行映射到`height-1-i`行

```python
test_image = np.load("test_image.npy")

flipped_image = vertical_flip(test_image)
show_images(np.stack([test_image, flipped_image]), labels=['Original', 'Flipped'], reshape=True)
```

<img src="/images/189_hw1_12.png" alt="label_plot" />

这里要说一下, 这个变换矩阵`T`的尺寸是`N*N`, 其中`N=height*width`, 可以理解为`height*width`个像素点, 每个像素点从`i*width + j`被映射到`(height-1-i)*width + j`


## Problem 4a
要实现水平翻转, 依葫芦画瓢而已, 行不变, 列从`j`变成`width - 1 - j`, 所以每个元素`i*width + j`变到`i*width + (width - 1 - j)`

```python
def create_horizontal_flip_matrix(height=28, width=28):
    """
    Returns a (height*width, height*width) matrix that horizontally flips an image
    when applied to its flattened vector. Values are 0 or 1.
    """
    N = height * width
    T = np.zeros((N, N), dtype=int)
    for i in range(height):
        for j in range(width):
            orig_idx = i * width + j
            flipped_j = width - 1 - j
            flipped_idx = i * width + flipped_j
            T[flipped_idx, orig_idx] = 1
    return T

    
def horizontal_flip(image):
    T_flip = create_horizontal_flip_matrix()
    return apply_transformation(image, T_flip)

flipped_image = horizontal_flip(test_image)

show_images(np.stack([test_image, flipped_image]), labels=['Original', 'Horizontal Flipped'], reshape=True)
```

<img src="/images/189_hw1_13.png" alt="label_plot" />

## Problem 4b 
要求实现图像的Shift移动, 想象一下, 如果是水平移动, 那就是列`j`被映射到`j + dx`(先不考虑左右的符号问题), 所以每个像素点从`i*width + j`被映射到`i*width + j + dx`


这里有点小trick, 就是把x和y先flatten一下然后再移动

```python
def create_shift_matrix(dx, dy, height=28, width=28):
    """
    Create a transformation matrix for shifting an image by dx pixels horizontally and dy pixels vertically.

    Args:
        dx (int): Number of pixels to shift horizontally.
        dy (int): Number of pixels to shift vertically.
        height (int): Height of the image.
        width (int): Width of the image.

    Returns:
        np.ndarray: A (height*width, height*width) transformation matrix for shifting.
    """
    N = height * width
    T = np.zeros((N, N))
    yy, xx = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')
    yy_flat = yy.ravel()
    xx_flat = xx.ravel()

    y_shift = yy_flat + dy
    x_shift = xx_flat + dx

    valid = (
        (y_shift >= 0) & (y_shift < height) &
        (x_shift >= 0) & (x_shift < width)
    )

    src = yy_flat[valid] * width + xx_flat[valid]
    dst = y_shift[valid] * width + x_shift[valid]

    T[dst, src] = 1.0

    return T


def shift_image(image, dx, dy):
    """
    Shift an image by dx pixels horizontally and dy pixels vertically.

    Args:
        image (np.ndarray): Flattened image array of shape (height*width,).
        dx (int): Number of pixels to shift horizontally.
        dy (int): Number of pixels to shift vertically.

    Returns:
        np.ndarray: Shifted image as a flattened array.
    """
    T = create_shift_matrix(dx, dy)
    return apply_transformation(image, T)

shifted_right_image = shift_image(test_image, 5, 0)
shifted_left_image = shift_image(test_image, -5, 0)
shifted_up_image = shift_image(test_image, 0, -5)
shifted_down_image = shift_image(test_image, 0, 5)

all_images = np.stack([test_image, shifted_up_image, shifted_down_image, shifted_right_image, shifted_left_image])
plot_labels = ['Original', 'Shifted Up', 'Shifted Down', 'Shifted Right', 'Shifted Left']
show_images(all_images, labels=plot_labels, reshape=True)
```

注意这上面的`xx` `yy`都有`height*weight`个元素, 他们本质上是所有元素的列坐标和行坐标, 所以说被flatten以后, 比如说`xx_flat + dx`就是对所有的列坐标都加上`dx`, 然后后面再用形如`i * width + height`的方式来还原

<img src="/images/189_hw1_14.png" alt="label_plot" />